{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T14:25:09.147187Z",
     "start_time": "2021-01-07T14:25:09.134198Z"
    }
   },
   "source": [
    "# 09. Ch 03. Probabilistic Perspective - 01. 들어가며\n",
    "- 다시 우리의 목표\n",
    "    - 가상의 함수를 모사해 원하는 출력 값을 반환하는 신경망의 파라미터를 찾자\n",
    "    - 그래서 우리는 딥뉴럴네트워크를 이야기할 떄, \n",
    "    - 그레디언트 디센트\n",
    "    - 백프로퍼게이션\n",
    "    - 피처 벡터\n",
    "    - 앤 블라블라\n",
    "    - 이제는 우리의 생각을 확장시켜야 할 때\n",
    "\n",
    "- 이 세상은 확률에 기반한다\n",
    "    - 아래의 그림에 대해서 모두가 같은 대답을 하지는 않을 것\n",
    "    - 토끼 vs 오리\n",
    "    - 우리의 새로운 목표: 확률 분포를 학습하는 것\n",
    "    - 비포 앤 애프터\n",
    "    - 비포 :함수를 배우자\n",
    "    - deterministic target 값을 예측\n",
    "\n",
    "- 애프터: 확률 분포 함수를 배우자\n",
    "    - 수학적으로 좀 더 설명 가능함\n",
    "    - 불확실성까지 학습\n",
    "\n",
    "- 요약\n",
    "    - 뉴럴 네트워크는 확률분포 함수를 모델링 할 수 있음\n",
    "    - 이를 통해 가상의 확률분포 함수P(y|x)를 근사(approximation)할 것\n",
    "    - 대부분의 최신 기술들은 이 관점에 기반을 두고 만들어짐\n",
    "\n",
    "- DNN(딥뉴럴네트워크)을 확률 분포로 보았을 때, 가능한 이론들에 대해서 앞으로 이야기 할 것\n",
    "    - Likelihood\n",
    "    - Maximum Likelihood Estimation(MLE)\n",
    "    - Maximum A Posterior (MAP) Estimation\n",
    "    - Cross Entropy & KL-Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Ch 03. Probabilistic Perspective - 02. 기본 확률 통계\n",
    "- 랜덤 variable(확률 변수) & Probability distribution(확률 분포) \n",
    "    - 어떤 변수 x가 x라는 값을 가질 확률\n",
    "    - P(x=x)\n",
    "    - 확률 분포(함수)\n",
    "    - 입력: 확률변수 x\n",
    "    - 출력: x가 각 값에 해당 될 때에 대한 확률 값\n",
    "\n",
    "- discrete probability distribution(이산 확률 분포)\n",
    "    - 확률 값의 총 합은 1, 확률은 0과 1 사이\n",
    "    - Probability Mass Function(확률 질량 함수, PMF)\n",
    "    - 1(1/6), 2(1.6),…,6(1.6)\n",
    "\n",
    "- continuous probability distribution(연속 확률 분포)\n",
    "    - Probability Density Function(확률 밀도 함수, PDF)\n",
    "    - 면적의 합의 1\n",
    "    - 함수 값이 1보다 클 수 있다.\n",
    "    - 연속확률변수의 경우 어떤 샘플이 주어졌을 때 확률 값을 알 수 없다\n",
    "\n",
    "- 결합확률(Joint Probability)\n",
    "    - 결합 분포\n",
    "    - P(x,y)\n",
    "\n",
    "- 조건부확률(Conditional Prob)\n",
    "    - 조건부 확률 분포\n",
    "    - P(y|X) = P(X,y) / P(X)\n",
    "    - 좀 더 친해져야 할 형태\n",
    "    - P(x,y) = P(y|X)P(X) = P(X|y)P(y)\n",
    "\n",
    "- 베이즈정리\n",
    "    - P(A|B) = P(B|A)P(A) / P(B)\n",
    "    - 데이터 D가 주어졌을 때, 가설 h의 확률\n",
    "    - P(h|D) = P(D|h)P(h) / p(D)\n",
    "\n",
    "- Function? or Value?(헷갈리기 쉬운 것) 함수? 값?\n",
    "    - 확률 값\n",
    "    - P(x), P(X=x)\n",
    "    - 확률 분포 함수\n",
    "    - P(X)\n",
    "    - 그럼 아래의 것들은?\n",
    "    - P(y|x) = P(Y=y|X=x)\n",
    "    - P(Y|x) = P(Y | X=x)\n",
    "    - P(y|X) = f(X)\n",
    "\n",
    "- 몬티홀 문제\n",
    "    - 확률 변수(Random Variables)\n",
    "    - A,B,C\n",
    "\n",
    "- Marginal Distribution(주변 확률)\n",
    "    - 결합 분포에서 한 변수를 적분한 형태\n",
    "\n",
    "- Expectaion(기대값) and 샘플링\n",
    "\n",
    "- EX: 주사위굴리기\n",
    "    - 주사위의 기대값은? 3.5\n",
    "    - 즉 기대값은 평균을 내는 건데 확률 값으로 가중 평균을 내는 것\n",
    "\n",
    "- 몬테카를로 샘플링\n",
    "    - 확률 분포로부터 샘플링을 통해 f의 가중 평균을 구해보자\n",
    "    - 몬테카를로 샘플링은 샘플링하는 횟수가 훨씬 많을수록 자세해지는 근사하는 방법\n",
    "    - 몬테카를로 샘플링을 통해서 f(x)의 가중평균을 근사해주는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Ch 03. Probabilistic Perspective - 03. Maximum Likelihood Estimation (MLE)\n",
    "- 대한민국 국민 신장의 분포를 알고 싶다\n",
    "    - 밖으로 나간다\n",
    "    - 지나가는 사람을 찾는다\n",
    "    - 붙잡고 키를 물어본다\n",
    "    - 충분한 샘플이 모일 때까지 반복한다\n",
    "    - 평균과 표준편차를 구한다\n",
    "\n",
    "- 가능도 함수\n",
    "    - 입력으로 주어진 확률 분포(파라미터)가 데이터를 얼마나 잘 설명하는지(확률 밀도가 크게 나왔다) 나타내는 점수(가능도)를 출력하는 함수\n",
    "    - 입력: 확률 분포를 표현하는 파라미터\n",
    "    - 출력: 데이터를 설명하는 정도\n",
    "    - 데이터를 잘 설명하는지 알 수 있는 방법 \n",
    "    - 데이터가 해당 확률 분포에서 높은 확률 값을 가질 것\n",
    "    - 그 말인 즉슨 그라운드 트루스 확률 분포가 이 확률 분포와 비슷하지 않을까, 잘 모사하고 있지 않을까 추측할 수 있는 것\n",
    "    - 세타라는 파라미터를 가지는 확률 분포가 데이터를 얼마나 잘 설명하고 있는가 = 최대가능도로 가장 최적의 세타를 구할 수 있다\n",
    "\n",
    "- 로그 라이클리후드\n",
    "    - 앞선 예제에서 볼 수 있듯이, 가능도는 확률 값의 곱으로 표현됨 -> 언더플로우의 가능성\n",
    "    - 따라서 로그를 취해 곱셈을 덧셈으로 바꾸고, 로그 라이클리후드로 문제를 해결\n",
    "    - 덧셈이 곱셈보다 연산도 빠름\n",
    "\n",
    "- MLE via Gradient Ascent\n",
    "    - 랜덤 생성 대신, 그레디언트 ascent를 통해 가능도 값을 최대로 만드는 파라미터 세타를 찾자\n",
    "    - 그전까지는 우리가 DNN 학습을 할 때는 로스를 미니마이즈 하기 위해서 그레디언트 디센트를 썼는데 여기선 최대화해야 하므로 그레디언트 어센트를 쓴다\n",
    "\n",
    "- 요약\n",
    "    - 우리는 가상의 확률 분포를 모사하는 확률 분포의 파라미터 세타를 찾고 싶다\n",
    "    - 목표 확률 분포로부터 데이터를 수집한 후, 데이터를 잘 설명하는 파라미터를 찾자.\n",
    "    - 가능도라는 값을 통해 얼마나 잘 설명하는지 알 수 있다\n",
    "    - 가능도 함수는 세타를 입력으로 받아, 데이터들의 세타에 대한 확률 값의 곱을 출력\n",
    "    - 가능도를 최대화하는 파라미터를 찾으면, 주어진 데이터를 가장 잘 설명한다\n",
    "    - 그레디언트 어센트를 통해 찾자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Ch 03. Probabilistic Perspective - 04. 신경망과 MLE\n",
    "- 모두 같은 표현, 관점이 조금씩 다를 뿐\n",
    "    - P세타(x) = P(x;세타) = P(x|세타)\n",
    "    - P세타(y|x) = P(y|x;세타) = P(y|x,세타)\n",
    "    - 세미콜론은 수식적으로 봤을 떄 조건부와 같다고 볼 수 있음\n",
    "\n",
    "- 우리의 목표는\n",
    "    - 확률 분포로부터 샘플링해 데이터를 넣었을 때, 확률 분포를 반환하는 가상의 함수를 모사하는 것.\n",
    "    - 출력 분포에서 샘플링하면 원하는 출력 값을 얻을 수 있다.\n",
    "    - 예: 손 글씨가 주어졌을 때, 글씨의 클래스의 확률 분포\n",
    "    - P(c|x), where x ~ P(X)\n",
    "\n",
    "- 리뷰: 최대 가능도 추정\n",
    "    - 우리는 가상의 확률 분포를 모사하는 확률 분포의 파라미터 세타를 찾고 싶다\n",
    "    - 목표 확률 분포로부터 데이터를 수집한 뒤 , 데이터를 잘 설명하는 파라미터를 찾자\n",
    "    - 가능도라는 값을 통해 얼마나 잘 설명하는지 알 수 있다\n",
    "    - 가능도 함수는 세타를 입력받아, 데이터들의 세타에 대한 확률 값의 곱을 출력\n",
    "    - 가능도를 최대화하는 파라미터를 찾으면, 주어진 데이터를 가장 잘 설명한다\n",
    "    - 그라디언트 어센트를 통해서 찾자(랜덤하게 나이브하게 찾는 방법도 있지만)\n",
    "\n",
    "- 확률 분포를 위한 파라미터\n",
    "    - 베르누이 분포 : 세타는 확률 p\n",
    "    - 가우시안 분포: 세타는 뮤와 시그마\n",
    "\n",
    "- DNN에서의 파라미터\n",
    "    - 가중치 W와 편향 bias\n",
    "    - 마찬가지로 그레디언트 어센트를 통해 가능도를 최대로 하는 파라미터 세타를 찾을 수 있다.\n",
    "\n",
    "- 네거티브 로그 라이클리후드(NLL)\n",
    "    - 하지만 대부분의 딥러닝 프레임워크들은 그레디언트 디센트만 지원\n",
    "    - 따라서 맥서마이제이션 문제에서 미니마이제이션 문제로 접근\n",
    "    - 마이너스 1을 곱하면 최소화 문제로 풀 수 있다\n",
    "\n",
    "- 딥뉴럴 네트워크 위드 MLE\n",
    "    - 분포 P(X)로부터 샘플링한 데이터 x가 주어졌을 떄, 파라미터 세타를 갖는 DNN은 조건부 확률 분포를 나타낸다.\n",
    "    - 이때, 우리는 그레디언트 디센트를 통해 NLL을 최소화하는 세타를 찾을 수 있다.\n",
    "\n",
    "- 요약\n",
    "    - MLE를 통해 수집한 데이터셋을 잘 설명하는 확률 분포의 마라미터를 찾을 수 있음\n",
    "    - 뉴럴 네트워크 또한 확률 분포함수이므로, MLE를 통해 파라미터를 찾을 것\n",
    "    - 최대화 대신 최소화를 위해 네거티브 로그 라이클리후드(NLL)를 그레디언트 디센트\n",
    "    - 그레디언트 디센트를 수행하기 위해선, 파라미터에 대한 미분이 필요함\n",
    "    - 이를 효율적으로 수행하기 위해 back-propagation을 활용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
